package data.spark.predict

import org.apache.spark.sql.{SparkSession, functions => F}

/**
 * ğŸ¯ åŠŸèƒ½ç›®æ ‡ï¼šæœªæ¥æ—¶é—´æ®µæ‹¥å µè¶‹åŠ¿å›¾
 * å¯¹åº” predict.html é¡µé¢ä¸­ä¸‹å·¦å›¾è¡¨ï¼ˆJS: time_trend.jsï¼‰ï¼Œå±•ç¤ºâ€œæ˜å¤©å…¨å¤©â€ä¸åŒæ—¶é—´æ®µçš„å¹³å‡æ‹¥å µæŒ‡æ•°è¶‹åŠ¿ã€‚
 * */

object TimeTrendGenerator {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Time Trend Generator")
      .master("local[*]")
      .getOrCreate()

    import spark.implicits._

    // Step 1: åŠ è½½æ•°æ®
    val df = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .csv("src/main/Scala/data/spark/predict_events.csv")

    // Step 2: è·å–æœ€æ–°é¢„æµ‹æ—¥æœŸ
    val latestDate = df.filter($"type" === "é¢„æµ‹")
      .agg(F.max("date")).as[String].collect()(0)

    val filtered = df.filter($"type" === "é¢„æµ‹" && $"date" === latestDate)

    // Step 3: æ¯å°æ—¶èšåˆ
    val hourAgg = filtered.groupBy("hour")
      .agg(F.round(F.avg("congestion_index"), 1).alias("index"))

    // Step 4: è‡ªå®šä¹‰å°æ—¶æ’åºé¡ºåº
    val hourOrder = Seq("6:00", "8:00", "10:00", "12:00", "14:00", "16:00", "18:00", "20:00", "22:00")

    val sorted = hourAgg.filter($"hour".isin(hourOrder: _*))
      .orderBy(F.when($"hour" === "6:00", 1)
        .when($"hour" === "8:00", 2)
        .when($"hour" === "10:00", 3)
        .when($"hour" === "12:00", 4)
        .when($"hour" === "14:00", 5)
        .when($"hour" === "16:00", 6)
        .when($"hour" === "18:00", 7)
        .when($"hour" === "20:00", 8)
        .when($"hour" === "22:00", 9)
      )

    // Step 5: æ”¶é›†ä¸ºä¸€æ¡ JSON è®°å½•
    val hourList = sorted.select("hour").as[String].collect().toList
    val indexList = sorted.select("index").as[Double].collect().toList

    val result = spark.createDataFrame(Seq(
      (hourList, indexList)
    )).toDF("hour", "index")

    result.coalesce(1)
      .write.mode("overwrite")
      .json("src/main/Scala/data/spark/predict/output")

    spark.stop()
  }
}

