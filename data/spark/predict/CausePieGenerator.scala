package data.spark.predict

import org.apache.spark.sql.{SparkSession, functions => F}

/**
 * ğŸ¯ åŠŸèƒ½ç›®æ ‡ï¼šæ‹¥å µåŸå› åˆ†æé¥¼å›¾
 * å¯¹åº” predict.html å·¦ä¸­å›¾è¡¨ï¼ˆJS: cause_pie.jsï¼‰ï¼Œéœ€è¦å±•ç¤ºæ¯ç§æ‹¥å µåŸå› åœ¨æ‰€æœ‰é¢„æµ‹è®°å½•ä¸­çš„å æ¯”ã€‚
 */
object CausePieGenerator {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Cause Pie Generator")
      .master("local[*]")
      .getOrCreate()

    import spark.implicits._

    // Step 1: åŠ è½½é¢„æµ‹æ•°æ®
    val df = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .csv("src/main/Scala/data/spark/predict_events.csv")

    // Step 2: è·å–é¢„æµ‹æ—¥æœŸä¸­çš„æœ€æ–°ä¸€æ—¥æ•°æ®
    val latestDate = df.filter($"type" === "é¢„æµ‹")
      .agg(F.max("date")).as[String].collect()(0)

    val filtered = df.filter($"type" === "é¢„æµ‹" && $"date" === latestDate)

    // Step 3: æŒ‰ cause åˆ†ç»„ç»Ÿè®¡æ•°é‡
    val causeCounts = filtered.groupBy("cause")
      .agg(F.count("*").alias("value"))
      .withColumnRenamed("cause", "name")

    // Step 4: è¾“å‡ºä¸º JSON æ–‡ä»¶
    causeCounts.coalesce(1)
      .write.mode("overwrite")
      .json("src/main/Scala/data/spark/predict/output/cause_pie")    // Step 4: è¾“å‡ºä¸º JSON æ–‡ä»¶
    causeCounts.coalesce(1)
      .write.mode("overwrite")
      .csv("src/main/Scala/data/spark/predict/output/cause_pie2")

    spark.stop()
  }
}

