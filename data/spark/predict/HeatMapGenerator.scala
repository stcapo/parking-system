package data.spark.predict

import org.apache.spark.sql.{SparkSession, functions => F}

/**
 * ğŸ¯ åŠŸèƒ½ç›®æ ‡ï¼šåŒºåŸŸæ‹¥å µçƒ­åŠ›å›¾
 * å¯¹åº” predict.html é¡µé¢å·¦ä¸Šè§’å›¾è¡¨ï¼Œæ˜¾ç¤ºæ¯ä¸ªåŒºåŸŸçš„æ‹¥å µé¢„æµ‹å¼ºåº¦ã€‚
 */
object HeatMapGenerator {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Heat Map Generator")
      .master("local[*]")
      .getOrCreate()

    import spark.implicits._

    // Step 1: åŠ è½½åŸå§‹é¢„æµ‹æ•°æ®
    val df = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .csv("src/main/Scala/data/spark/predict_events.csv")

    // Step 2: è¿‡æ»¤â€œé¢„æµ‹â€ç±»å‹ï¼Œå–æœ€è¿‘é¢„æµ‹æ—¥æœŸï¼ˆé€šå¸¸æ˜¯æ˜å¤©ï¼‰
    val latestDate = df.filter($"type" === "é¢„æµ‹")
      .agg(F.max("date")).as[String].collect()(0)

    val predictOnly = df.filter($"type" === "é¢„æµ‹" && $"date" === latestDate)

    // Step 3: æ¯ä¸ª region çš„å¹³å‡æ‹¥å µæŒ‡æ•°
    val regionHeat = predictOnly.groupBy("region")
      .agg(F.round(F.avg("congestion_index"), 1).alias("index"))
      .orderBy($"index".desc)

    // Step 4: è¾“å‡ºä¸º JSON
    regionHeat.coalesce(1)
      .write.mode("overwrite")
      .json("src/main/Scala/data/spark/predict/output/heat_map")    // Step 4: è¾“å‡ºä¸º JSON
    regionHeat.coalesce(1)
      .write.mode("overwrite")
      .csv("src/main/Scala/data/spark/predict/output/heat_map2")

    spark.stop()
  }
}

