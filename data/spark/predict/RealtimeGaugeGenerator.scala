package data.spark.predict

import org.apache.spark.sql.{SparkSession, functions => F}

/**
 * ğŸ¯ åŠŸèƒ½ç›®æ ‡ï¼šå®æ—¶æ‹¥å µæŒ‡æ•°ä»ªè¡¨ç›˜
 * å¯¹åº” predict.html å·¦ä¸‹å›¾è¡¨ï¼ˆJS: realtime_gauge.jsï¼‰ï¼Œéœ€è¦æ˜¾ç¤º â€œå½“å‰æ—¶é—´ç‚¹â€æˆ–é¢„æµ‹æ—¶é—´ç‚¹çš„ç»¼åˆæ‹¥å µæŒ‡æ•°ã€‚
 */

object RealtimeGaugeGenerator {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Realtime Gauge Generator")
      .master("local[*]")
      .getOrCreate()

    import spark.implicits._

    // Step 1: åŠ è½½æ•°æ®
    val df = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .csv("src/main/Scala/data/spark/predict_events.csv")

    // Step 2: è·å–â€œé¢„æµ‹â€ä¸­æœ€è¿‘ä¸€å¤©çš„æ•°æ®ï¼ˆå³æ˜å¤©ï¼‰
    val latestDate = df.filter($"type" === "é¢„æµ‹")
      .agg(F.max("date")).as[String].collect()(0)

    val filtered = df.filter($"type" === "é¢„æµ‹" && $"date" === latestDate)

    // Step 3: è®¡ç®—æ•´ä½“å¹³å‡æ‹¥å µæŒ‡æ•°ï¼ˆå…¨å¸‚é¢„æµ‹å€¼å¹³å‡ï¼‰
    val avgIndex = filtered.agg(F.round(F.avg("congestion_index"), 1)).as[Double].collect()(0)

    // Step 4: æ„é€  DataFrame å•è¡Œè¾“å‡º
    val now = java.time.LocalDateTime.now().toString.replace("T", " ").substring(0, 19)
    val result = Seq((now, avgIndex)).toDF("time", "index")

    result.coalesce(1)
      .write.mode("overwrite")
      .json("src/main/Scala/data/spark/predict/output/realtime_gauge")

    spark.stop()
  }
}

